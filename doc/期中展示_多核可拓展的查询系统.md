# Scalable and Efficient Spatial Data Management on Multi-Core CPU and GPU Clusters: A Preliminary Implementation based on Impala
<center>2015311965 康荣</center>
本文的关键词是：空间数据、多核可拓展、impala（一个开源的数据存储与查询系统）

## 概述
作者文中提到，当前估计有80%的互联网产生数据都包含空间或地理位置信息成分。智能手机产生越来越多的GPS数据，以及WIFI或流量定位数据，这些都包含了空间成分。随着空间数据的应用与需求日益增加，以及高性能的传感器的发展，空间大数据已经成为了大数据的重要部分。同时，如何能更高效、并且可拓展的处理空间数据的存储、查询等也受到关注。

本文针对于大数据量的空间数据，在impala（一种开源的大数据系统）之上，设计实现了ISP-MC+（针对多核可拓展性进行了优化）以及ISP-GPU（基于GPU的优化）。二者都在十核的Amazon EC2集群上进行了测试，展现了高性能和高可拓展性。

已有的技术都关注于拓展性（HadoopGIS以及SpatialHadoop）或者单机性能（如基于R-tree的内存索引），本文同时考虑了两者，基于impala拓展实现了ISP系统（In-memory Spatial Processing）。大多数ISP的模块是基于Thrust并行库来设计的，该库允许我们在multi-core和GPU上采用相同的实现方式。

impala是一种针对大规模数据的、并行执行的SQL查询引擎，他的底层是建立在HDFS之上的，可以说天生的支持分布式和可拓展。同时，它的实现方式不同于mapreduce（涉及频繁的IO操作，耗费了大量时间）。

ISP的前端是基于impala的，impala可以解析空间查询SQL并生成逻辑查询计划和物理查询计划。同时采用impala的后端来支持分布式计算。实验中ISP展现了良好的性能。同时，由于impala是为了关系型数据设计的，而我们的实验是面向空间数据的，二者结合产生的良好效果让我们有理由相信，本文的工作会对那些支持半结构化数据的系统产生参考价值。

## 空间数据的一些相关工作
以往在大规模空间数据的探索中，基本上是基于hadoop/mapreduce的。如（Hadoop-GIS [1], SpatialHadoop [2] and ESRI GIS Tools for Hadoop [9]）。同时，由于现在内存性能得到了大的发展，因此许多设计为了达到高性能、减少IO，多在内存处理数据以突破IO瓶颈（如Apache Spark和Cloudera Impala）。

传统的MapReduce程序都包含大量的IO操作，性能较低。impala是基于Hadoop生态环境开发的，支持HDFS（分布式文件系统），在内存中处理大量数据提升性能。他的前端可以解析SQL语句并基于规则生成**逻辑查询计划**。同时，基于元数据和硬件配置信息（包括是否有索引、数据聚集/非聚集情况等），可以生成高效的**物理查询计划**。

impala还有两方面提供了较好的性能：1.采用C/C++编写。C/C++应该算是高效利用硬件资源的极佳选择了，同时impala是作者所指的唯一一个可以实时编译运行的系统（基于LLVM）。实时编译的影响重大，因为它可以支持不同种类的动态代码优化，以及制定灵活的UDFs(User Defined Functions)。

## ISP系统设计与实现
为了提供对空间查询的高性能处理以及对multi-core的 Cpu 和 Gpu支持，文章进行了三种主要扩展：

1. 第一，我们修改impala前端的抽象语法树 (AST) 模块，以支持空间查询；
2. 第二，我们将空间数据用字符串来表示，用以支持impala；
3. 第三，我们将原本的基于GPU的单机空间数据管理技术和impala集群进行了结合，用于支持大型空间数据处理与集成。

GPU的代码是基于CUDA、Thrust并行库以及Intel TBB库的。ISP的组件的体系结构如图1所示。

![](http://i3.piimg.com/135661421e85eddb.png)
ISP采用WKT（Well-Known Text）格式来表示空间数据。

>WKT，是一种文本标记语言，用于表示矢量几何对象、空间参照系统及空间参照系统之间的转换。它的二进制表示方式，亦即WKB(well-known binary)则胜于在传输和在数据库中存储相同的信息。该格式由开放地理空间联盟(OGC)制定。

ISP在查询的时候支持两种方式：1.single-sided查询；2.double-sided查询。前者是一种较短的查询，SQL语句中通常只有一个或几个对象。后者则是进行两张表的连接操作（join）。对于两个表（长度分别为m和n）的连接操作，空间复杂度可能达到O(m*n)。因此， ISP-GPU将所有的空间索引、空间过滤等都交给了GPU，以利用GPU的大规模并行计算能力。在实验部分可以看到，16-core Intel CPUs的ISP-MC+的性能通常比单节点的ISP-GPU要慢（GPU：NVIDIA GTX Titan GPU），但是两者结合的效果很出众。

下面是两种执行方式的对比：1.单独采用impala来进行table join；2.ISP的table join。

### impala
1. 首先读入SQL查询并分解。impala将其转换为一个AST(抽象语法树)，impala开始自顶向下的分解语法树，并将各部分分配给一个**计算节点**。当连接两张表的时候，AST的节点奖对应两个子节点，两个节点各自维护了一张表的必须的信息，以及WHERE子句中的一些限制条件。
2. impala请求两个节点的批处理操作，用以迭代地执行连接，并且递归的执行子查询。在连接中，左边的节点是**构造子结构**，右边的节点是**探测子结构**。考虑右表待连接的数据是按节点隔离分区，或是需要广播（不确定在哪个节点上，因此需要向所有节点广播），决定了不同的检索开销，可能遍历整张右表或者只是检索其中一部分。
3. 在impala 2.0中，左表会做分区处理来对并行性和分布式计算进行优化。这样的设计天生的倾向于不对称查询，即左表大（分区隔离），右表小（广播）。
4. 这样的场景在真实世界中有很多，例如，出租车客人上车和下车地点的数据增长很快，而相对应的，城市的道路交通是一张小表，我们要将他二者结合。
5. ISP基于GPUs的执行方式如下。首先，迭代检索小表一列的数据，并且为所有的元组建立索引。impala中从HDFS上检索小表可以很有效的使用多线程优化。第二，将左表的所有行迭代地进行批处理。对已每一行，我们使用GPU来并行的估计。由于空间估计是计算密集型，同时对于小表只涉及读操作，可以利用GPU并行拓展，因此这一步将大大提高效率。最后，将查询结果返回CPU，这些结果或直接输出或者聚合输出。

首先，对于GPU的数据优化，SoA要比AoS要好。我们将左右表的几何图形都已二进制数组的行书存储，包括x/y点、复杂图形边界等等。将SoA形式组织的数据传入GPU，处理性能更好；其次，对右表进行索引，然后左表放入GPU复用可以提高效率；

CPU和GPU的参数设置大小存在一个权衡的问题。CPU是顺序执行而GPU可以并行执行。impala对于CPU的优化是，将被分到同一个实例中执行的各个元组进行流水线执行，这样可以减小中间结果的内存占用，如果需要较小的内存占用，则每一行row batch可以设置的较小。但是，每一行的row batch如果设置过小，则无法充分利用GPUs的性能。

## 性能测试
我们采用的point-in-polygon测试执行如下图：

![](http://i3.piimg.com/80931afc8a567b71.png)

左表是一堆点的集合（大表），右边是一个多边形的集合（小表，建立了R-tree索引）
则查询的SQL语句如下：

<center>
>SELECT point.id, polygon.id

>FROM point SPATIAL JOIN polygon 

>WHERE ST_WITHIN(point.geom, polygon.geom)
</center>

使用的算法是ray-tracing算法，复杂度是O(V)，V为多边形节点数。显然基于该测试的表连接是计算密集型数据，同时很适合GPU加速。

### 实验数据源
第一个实验：点集合是2013年NYC出租车拉客地点（170M，6.9GB），多边形来源于Census 2010（40k，18.7MB）。多边形平均端点数是9;

第二个实验：点集是GBIF库（375million，12.9GB），多边形是WWFG的生态划分区域数据（14458，149.8MB）。平均端点数是279.

实验机器：

10-node Amazon EC2 g2.2xlarge GPU cluster. 8-core Intel Sandy Bridge 2.6 GHZ CPUs, 128 GB memory, 8 TB HDD and an NVIDIA GTX TITAN GPU. 
GTX TITAN GPU： 6 GB GDDR5 memory , 2,668 CUDA cores. 
All Amazon g2.2xlarge instances (computing nodes) are equipped with 8 vCPU (Intel Sandy Bridge 2.6 GHZ), 15 GB memory, 60 GB SSD and an NVIDIA GPU with 4 GB graphics memory and 1,536 CUDA cores. 
操作系统： CentOS 6.5 and Hadoop 2.5.0 from Cloudera CDH 5.2.0 with default settings. 
HDFS data block size：64 MB by default.

两组实验，单机性能和集群的可拓展性。

在单机实验中，结果如下表：

|Experiment|ISP-GPU|ISP-MC+|GPU-Standalone|MC-Standalone|
|---|---|---|---|---|
|taxi-nycb (s)|96|130|50|89||GBF-WWF(s)|1822|2816|1498|2664|

可以看到GPU加速相对明显。同时，为了检测impala的解析、构造、查询等所占用的时间，这里实现了一种版本standalone，直接操作处理好的原始数据，而省去语法树解析、多核分配等过程。也可以看出，GPU的impala加载占用了46秒/324秒。这意味着随着数据的增大，这些基础影响会减弱，性会更好的体现出来。

第二个实验结果如下：

![](http://i2.piimg.com/ceaa4e8737d22246.png)

左图中有良好的可拓展性，而右图的可拓展性有所弱化。可能的原因是，当核数增多时，每个机器所分配的数据处理减少，会引起使用不满和负载不均衡等问题。这对于GPU和CPU多核都有影响，但是对于GPU影响更大（因为GPU的处理单元通常远大于CPU核数）。

最后也可以看出，GPU的加速效果似乎比相匹配的CPU加速效果要好。

## 论文相关工作及相关知识拓展
### WKT格式：
对于Geometry的著名文本（WKT）表示，它是为与采用ASCII格式的几何数据进行交换而设计的。
几何对象WKT表示的示例：

* Point：POINT(15 20)

>注意，指定点坐标时不使用分隔用逗号。

* 具有4个点的LineString：LINESTRING(0 0, 10 10, 20 25, 50 60)

>注意，点坐标对采用逗号隔开。

* 具有1个外部环和1个内部环的Polygon：

> POLYGON((0 0,10 0,10 10,0 10,0 0),(5 5,7 5,7 7,5 7, 5 5))

* 具有三个Point值的MultiPoint：

> MULTIPOINT(0 0, 20 20, 60 60)

* 具有2个LineString值的MultiLineString：

> MULTILINESTRING((10 10, 20 20), (15 15, 30 15))

* 具有2个Polygon值的MultiPolygon：

> MULTIPOLYGON(((0 0,10 0,10 10,0 10,0 0)),((5 5,7 5,7 7,5 7, 5 5)))

* 由2个Point值和1个LineString构成的GeometryCollection：

> GEOMETRYCOLLECTION(POINT(10 10), POINT(30 30), LINESTRING(15 15, 20 20))

### GPU的并行性能优化 
目前顶尖的GPU的浮点数计算能力远超过同等级的CPU。是因为GPU和CPU起初是为了不同的目的设计的。

CPU设计之初所负责的是如何把一条一条的数据处理完，CPU的内部结构可以分为控制单元、逻辑单元和存储单元三大部分，三个部分相互协调，便可以进行分析，判断、运算并控制计算机各部分协调工作。其中运算器主要完成各种算术运算(如加、减、乘、除)和逻辑运算( 如逻辑加、逻辑乘和非运算); 而控制器不具有运算功能,它只是读取各种指令,并对指令进行分析,作出相应的控制。通常,在CPU中还有若干个寄存器,它们可直接参与运算并存放运算的中间结果。CPU的工作原理就像一个工厂对产品的加工过程：进入工厂的原料（程序指令），经过物资分配部门（控制单元）的调度分配，被送往生产线（逻辑运算单元），生产出成品（处理后的数据）后，再存储在仓库（存储单元）中，最后等着拿到市场上去卖（交由应用程序使用）。在这个过程中，从控制单元开始，CPU就开始了正式的工作，中间的过程是通过逻辑运算单元来进行运算处理，交到存储单元代表工作的结束。数据从输入设备流经内存，等待CPU的处理。

而GPU却从最初的设计就能够执行并行指令，从一个GPU核心收到一组多边形数据，到完成所有处理并输出图像可以做到完全独立。由于最初GPU就采用了大量的执行单元，这些执行单元可以轻松的加载并行处理，而不像CPU那样的单线程处理。另外，现代的GPU也可以在每个指令周期执行更多的单一指令。例如，在某些特定环境下，Tesla架构可以同时执行MAD+MUL or MAD+SFU。

![](http://i4.piimg.com/aadf9edfa9ba8b53.jpg)

可以看到，GPU中的ALU计算单元远远超过CPU。CPU中大部分是缓存单元和控制单元，因此可以进行复杂的逻辑跳转操作，而GPU更倾向于快速、高并发的执行计算。


