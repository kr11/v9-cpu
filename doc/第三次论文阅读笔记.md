# 第三次论文研读
<center>康荣 2015311965</center>
## Non-scalable locks are dangerous
一些操作系统使用不可拓展的spin锁来序列化，这是很危险的，

众所周知，简单地spin锁是不可拓展性的所，这在多核的系统上性能非常差。一个系统在N核上性能很好，但是远低于N台机器性能之和，这种线性不可加性就是不可拓展性的体现。本文提出了一个不可拓展锁性能预测模型来解释这一现象。

本文的贡献在于，核实并放大了之前论文的结论，不可拓展锁不仅仅影响性能，还可能导致整个系统性能的崩溃。我们对linux kernel的spin lock进行了试验，benchmark是FOPS，MEMPOP，PFIND和EXIM。四个工作流的曲线非常像，首先有一段线性的增长，然后崩溃，FOPS在48核是单核性能只有3%。

本文提出了一个模型来子安稀罕不可拓展锁的新能。首先，高层级里有一个硬件缓存连贯性协议，它为每一个DRAM区域分配了目录：

```
[tag|state|core ID]
```
当core访问到未缓存的行时，core将会开始寻找，依照请求的不同目录直到找到并缓存，这可能遍历所有的core，非常耗时。

我们以 ticket locks为例来看性能，由于之前问题的存在（hardware cache coherence），整个过程非常耗时。实验结果很好地支持了模型的预测。

<img src='http://media.xtwind.com/images/2016/03/28/6848dce4e7c232914e89ed87c0f96398.png'/>

事实上，之前的许多论文中有许多可拓展的所，这可以避免像之前所显示的性能崩溃。ticket lock的拓展瓶颈可以使用两种手段，第一是修改ticket lock的实现，第二是换用其他可拓展的锁，包括MCS lock，K42 lock，CLH lock以及HCLH lock等等。在是沿途中也可以看到，这些可拓展锁的单核性能在核数增加时保持稳定。

<img src='http://media.xtwind.com/images/2016/03/28/2ea2a7befd6a0d9fbd65f564f4fa9c83.png'/>



## Scalable Kernel TCP Design and Implementation for Short-Lived Connections
由于飞速增长的带宽，CPU核数的增加，本文提出了一种新的，适应于短连接的、可拓展的TCP栈：Fastsocket，一种兼容BSD-socket的设计.

这个问题的主要来源在于，当前带宽渐增，网速渐增，每次传输的数据量也渐增，但是每次连接时间却很短（如微博），因此，快速的建立和终止连接就变得很重要。同时，由于核数增加，socket的可拓展性变得越发重要。**这里可以看到，还是核数的问题**

在理想情况下，单次连接的所有操作都在单CPU上完成，这样可以非常充分的发挥并行性。（可惜现在的linux上是有不同CPU完成的）

TCB负责连接建立断开。同时，一个连接的TCB在两阶段被更新：

1. 当CPU收到一个MIC中断，并传出了一个packet的时候；
2. 用户态的其他擦做以及执行CPU得到了输出packet准备向外发送的时候。

VFS抽象了一个socket，并向用户级别应用输出了一个socket（FD）。因此VFS在开关FD时候的开销极大地影响了connect的建立。

因此，socket性能瓶颈主要关注两点：TCB management and VFS abstraction。本文提出了Fastsocket，向后兼容的结局了socket的可拓展性和兼容性问题，建立可一个高可拓展性的kernel框架。（*对于当前的网络环境，向后兼容无疑是新技术投入应用最重要的需求*）

<img src='http://media.xtwind.com/images/2016/03/28/d681fd9e64e61a3439f60bee50c6d9dc.png'/>

Fastscoket主要包含三个部分：分区式TCB数据结构，RFD,以及Fastsocket形式下的VFS。这三个部分协作提供了单核执行的connection建立过程。我们使用局部监听表和局部简历表来实现表级别的分区，这可以用来建立socket的管理机制和监听已经建立的socket。

同时，我们设计了RFD(Receive Flow Deliver)解决局部的活跃连接的问题。我们使用了一个hash函数来讲port映射到不同的CPU核上。c = hash(port.src)，将port对应到唯一的core上。

在之前提到的FD问题上，一般来说socket是有VFS来管理，FD和其他的file类型类似。但是我们知道，socket和其他磁盘上的文件有很大的不同。首先socket并没有存在disk上，而且directory path没有用，因为不会用它来识别socket。因此，我们使用fastsocket-aware VFS来避免一些无用的消耗，从而加快速度。同时，我们我们不能去掉传统socket的所有变量（dentry和inode），否则将无法向后阿兼容。（举例来说，去掉它，netstat就无法工作了）

以上就是Fastsocket三部分的实现。

实验测试环境：
environment:
http_clint-->HAproxy-->不同类型的server
proxy的特征是：既是一个serber，又是一个client。
因此我们需要看到client的优化，最大的瓶颈就是proxy。
CPU：multi-core+SMP

CPU占了85%，但是内核态花了77%的时间在spin_lock；
在性能上来说，其实12核之前线性提升，但是之后就崩了。

>what to do
1.硬件提升？
2.TCP-IP协议栈可以自己设计但是不能用于生产场景里。
达不到想kernel这样完备的。
3.改kernel：一种是没改变接口，这样产业界认可；另一种：直接修改api。

non local process of connections

建立解除的时候占了很多：1.receive的时候interupt；2.应用和系统调用，这两个可能是在不同的cpu上，就出现了共享等等。

listen socket table是全局的，共享变量的锁机制；
VFS：一种抽象，可以适用不同的文件系统，但是socket不是fd，没有目录等等。

数据在不同的路径的问题，通过receive flow deliver，保证了数据一定在一个core上，使得不需要做同步。
执行路径进行了修改。
data partition：




